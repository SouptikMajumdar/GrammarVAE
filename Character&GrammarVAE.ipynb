{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 2.1.2+cu118\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "print(\"Using torch\", torch.__version__)\n",
    "import seaborn as sns\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "sns.set()\n",
    "torch.manual_seed(42) # Setting the seed\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n",
      "Device cuda\n",
      "Device cuda\n"
     ]
    }
   ],
   "source": [
    "from ac_dll_grammar_vae import print_contact_info\n",
    "from ac_dll_grammar_vae.data import CFGEquationDataset\n",
    "from ac_dll_grammar_vae.data.alphabet import alphabet\n",
    "from ac_dll_grammar_vae.data.transforms import MathTokenEmbedding, RuleTokenEmbedding, ToTensor, Compose, PadSequencesToSameLengthV2, OneHotEncode\n",
    "from train import *\n",
    "from eval import *\n",
    "from visualize import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device\", device)\n",
    "\n",
    "# GPU operations have a separate seed we also want to set\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Additionally, some operations on a GPU are implemented stochastic for efficiency\n",
    "# We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "#device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Embedding:\n",
    "+ Using Math Token Embedding Class\n",
    "\n",
    "{'+': 1,\n",
    " '-': 2,\n",
    " '3': 3,\n",
    " 'sqrt': 4,\n",
    " 'exp': 5,\n",
    " '(': 6,\n",
    " 'sin': 7,\n",
    " '/': 8,\n",
    " 'x': 9,\n",
    " ')': 10,\n",
    " '*': 11,\n",
    " 'log': 12,\n",
    " 'cos': 13,\n",
    " '2': 14,\n",
    " '1': 15,\n",
    " ' ': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'+': 1,\n",
       " '(': 2,\n",
       " '*': 3,\n",
       " 'x': 4,\n",
       " '1': 5,\n",
       " '3': 6,\n",
       " 'sin': 7,\n",
       " 'sqrt': 8,\n",
       " ')': 9,\n",
       " 'cos': 10,\n",
       " '-': 11,\n",
       " 'log': 12,\n",
       " '/': 13,\n",
       " '2': 14,\n",
       " 'exp': 15,\n",
       " ' ': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = MathTokenEmbedding(alphabet=alphabet)\n",
    "emb.token_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Data ['cos', '(', '(', '1', ')', ')', '+', '2']\n",
      "Encoded Example: Data [10, 2, 2, 5, 9, 9, 1, 14]\n"
     ]
    }
   ],
   "source": [
    "#Example Data\n",
    "data = CFGEquationDataset()\n",
    "#Example Encoding:\n",
    "print(f'Example: Data {data[42]}')\n",
    "encoded_data = emb.embed(data[42])\n",
    "print(f'Encoded Example: Data {encoded_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Example: Data ['cos', '(', '(', '1', ')', ')', '+', '2']\n",
    "+ Encoded Example: Data [13, 6, 6, 15, 10, 10, 1, 14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Training Dataset using CFG \n",
    " + Class CFGEquationDataset is used to generate the equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = CFGEquationDataset(\n",
    "        n_samples=100000,\n",
    "        transform=Compose([\n",
    "            MathTokenEmbedding(alphabet),\n",
    "            ToTensor(dtype=torch.uint8)\n",
    "        ]))\n",
    "\n",
    "#Batch Size:\n",
    "batch_size = 100\n",
    "MAX_SEQ_LEN = 21\n",
    "collate_fn = PadSequencesToSameLengthV2(padding_value=0, max_length=21)\n",
    "training_loader = DataLoader(dataset=training,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=PadSequencesToSameLengthV2(padding_value=0, max_length=21))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the Embedded equations into one hot encoded vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100000, 21, 16])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehotencoder = OneHotEncode(alphabet)\n",
    "one_hot_encoded_training = []\n",
    "for batch in training_loader:\n",
    "  try:\n",
    "    one_hot_encoded_batch = onehotencoder(batch)\n",
    "  except Exception as e:\n",
    "    print(batch)\n",
    "    continue\n",
    "  one_hot_encoded_training.append(one_hot_encoded_batch.numpy())\n",
    "one_hot_encoded_training = np.array(one_hot_encoded_training)\n",
    "one_hot_encoded_training_tensor = torch.Tensor(one_hot_encoded_training)\n",
    "one_hot_encoded_training_tensor = one_hot_encoded_training_tensor.view(one_hot_encoded_training_tensor.shape[0]*one_hot_encoded_training_tensor.shape[1],one_hot_encoded_training_tensor.shape[2],one_hot_encoded_training_tensor.shape[3])\n",
    "one_hot_encoded_training_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating final dataloader for model which is one-hot-encoded:\n",
    " + Setting Maximum equation length to 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "MAX_SEQ_LEN = 21\n",
    "alphabet_length = len(alphabet) + 1\n",
    "one_hot_encoded_training_loader = DataLoader(dataset=one_hot_encoded_training_tensor,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Test Dataset and Dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CFGEquationDataset(\n",
    "        n_samples=1000,\n",
    "        transform=Compose([\n",
    "            MathTokenEmbedding(alphabet),\n",
    "            ToTensor(dtype=torch.uint8)\n",
    "        ]))\n",
    "#Batch Size:\n",
    "batch_size = 100\n",
    "MAX_SEQ_LEN = 21\n",
    "collate_fn = PadSequencesToSameLengthV2(padding_value=0, max_length=21)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=PadSequencesToSameLengthV2(padding_value=0, max_length=21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 21, 16])\n"
     ]
    }
   ],
   "source": [
    "one_hot_encoded_testing = []\n",
    "for batch in test_loader:\n",
    "  try:\n",
    "    one_hot_encoded_batch = onehotencoder(batch)\n",
    "  except Exception as e:\n",
    "    print(batch)\n",
    "    continue\n",
    "  #print(one_hot_encoded_batch.shape)\n",
    "  one_hot_encoded_testing.append(one_hot_encoded_batch.numpy())\n",
    "one_hot_encoded_testing = np.array(one_hot_encoded_testing)\n",
    "one_hot_encoded_testing_tensor = torch.Tensor(one_hot_encoded_testing)\n",
    "one_hot_encoded_testing_tensor = one_hot_encoded_testing_tensor.view(one_hot_encoded_testing_tensor.shape[0]*one_hot_encoded_testing_tensor.shape[1],one_hot_encoded_testing_tensor.shape[2],one_hot_encoded_testing_tensor.shape[3])\n",
    "print(one_hot_encoded_testing_tensor.shape)\n",
    "one_hot_encoded_testing_loader = DataLoader(dataset=one_hot_encoded_testing_tensor,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization for Character AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import EqnAE\n",
    "from train import train_AEmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 0 Average Training loss: 70.77181650\n",
      "====> Epoch: 0 Average Validation loss: 69.16035659\n",
      "====> Epoch: 1 Average Training loss: 68.53184343\n",
      "====> Epoch: 1 Average Validation loss: 68.13218423\n",
      "====> Epoch: 2 Average Training loss: 67.67414947\n",
      "====> Epoch: 2 Average Validation loss: 67.55131982\n",
      "====> Epoch: 3 Average Training loss: 67.20789350\n",
      "====> Epoch: 3 Average Validation loss: 67.20708716\n",
      "====> Epoch: 4 Average Training loss: 66.87045394\n",
      "====> Epoch: 4 Average Validation loss: 66.81307749\n",
      "====> Epoch: 5 Average Training loss: 66.61108429\n",
      "====> Epoch: 5 Average Validation loss: 66.67391694\n",
      "====> Epoch: 6 Average Training loss: 66.37322133\n",
      "====> Epoch: 6 Average Validation loss: 66.44243198\n",
      "====> Epoch: 7 Average Training loss: 66.19692827\n",
      "====> Epoch: 7 Average Validation loss: 66.26117349\n",
      "====> Epoch: 8 Average Training loss: 66.03364886\n",
      "====> Epoch: 8 Average Validation loss: 66.13133955\n",
      "====> Epoch: 9 Average Training loss: 65.91795974\n",
      "====> Epoch: 9 Average Validation loss: 66.07275942\n",
      "====> Epoch: 10 Average Training loss: 65.73287780\n",
      "====> Epoch: 10 Average Validation loss: 65.88047178\n",
      "====> Epoch: 11 Average Training loss: 65.63987356\n",
      "====> Epoch: 11 Average Validation loss: 65.86370146\n",
      "====> Epoch: 12 Average Training loss: 65.57254621\n",
      "====> Epoch: 12 Average Validation loss: 65.71387012\n",
      "====> Epoch: 13 Average Training loss: 65.42346314\n",
      "====> Epoch: 13 Average Validation loss: 65.84941377\n",
      "====> Epoch: 14 Average Training loss: 65.36312402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/17 19:07:56 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 14 Average Validation loss: 65.70504473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/17 19:08:08 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.16.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torchvision==0.16.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10)\n",
      "====> Epoch: 0 Average Training loss: 72.29415484\n",
      "====> Epoch: 0 Average Validation loss: 71.68044658\n",
      "====> Epoch: 1 Average Training loss: 70.61529671\n",
      "====> Epoch: 1 Average Validation loss: 70.28687285\n",
      "====> Epoch: 2 Average Training loss: 69.62725624\n",
      "====> Epoch: 2 Average Validation loss: 69.36927588\n",
      "====> Epoch: 3 Average Training loss: 69.00852592\n",
      "====> Epoch: 3 Average Validation loss: 69.00159707\n",
      "====> Epoch: 4 Average Training loss: 68.05498070\n",
      "====> Epoch: 4 Average Validation loss: 68.29976904\n",
      "====> Epoch: 5 Average Training loss: 67.47255576\n",
      "====> Epoch: 5 Average Validation loss: 67.45084902\n",
      "====> Epoch: 6 Average Training loss: 67.21568488\n",
      "====> Epoch: 6 Average Validation loss: 67.26053916\n",
      "====> Epoch: 7 Average Training loss: 67.00963681\n",
      "====> Epoch: 7 Average Validation loss: 67.03398184\n",
      "====> Epoch: 8 Average Training loss: 66.84408646\n",
      "====> Epoch: 8 Average Validation loss: 66.89670449\n",
      "====> Epoch: 9 Average Training loss: 66.67709169\n",
      "====> Epoch: 9 Average Validation loss: 66.80415381\n",
      "====> Epoch: 10 Average Training loss: 66.55539781\n",
      "====> Epoch: 10 Average Validation loss: 66.66281797\n",
      "====> Epoch: 11 Average Training loss: 66.41056974\n",
      "====> Epoch: 11 Average Validation loss: 66.57812998\n",
      "====> Epoch: 12 Average Training loss: 66.30390854\n",
      "====> Epoch: 12 Average Validation loss: 66.98044053\n",
      "====> Epoch: 13 Average Training loss: 66.20860813\n",
      "====> Epoch: 13 Average Validation loss: 66.37010107\n",
      "====> Epoch: 14 Average Training loss: 66.03274536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/17 19:14:40 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 14 Average Validation loss: 66.33570889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/17 19:14:49 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.16.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torchvision==0.16.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "c:\\Users\\soupt\\anaconda3\\envs\\MPL\\lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\soupt\\anaconda3\\envs\\MPL\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10)\n",
      "====> Epoch: 0 Average Training loss: 70.43864831\n",
      "====> Epoch: 0 Average Validation loss: 68.71134014\n",
      "====> Epoch: 1 Average Training loss: 68.03017476\n",
      "====> Epoch: 1 Average Validation loss: 67.62979146\n",
      "====> Epoch: 2 Average Training loss: 67.18655681\n",
      "====> Epoch: 2 Average Validation loss: 67.11268047\n",
      "====> Epoch: 3 Average Training loss: 66.69171313\n",
      "====> Epoch: 3 Average Validation loss: 67.24404849\n",
      "====> Epoch: 4 Average Training loss: 66.41308485\n",
      "====> Epoch: 4 Average Validation loss: 66.46257402\n",
      "====> Epoch: 5 Average Training loss: 66.12420164\n",
      "====> Epoch: 5 Average Validation loss: 66.27077388\n",
      "====> Epoch: 6 Average Training loss: 65.89856599\n",
      "====> Epoch: 6 Average Validation loss: 66.01782075\n",
      "====> Epoch: 7 Average Training loss: 65.79825651\n",
      "====> Epoch: 7 Average Validation loss: 66.00281406\n",
      "====> Epoch: 8 Average Training loss: 65.54786136\n",
      "====> Epoch: 8 Average Validation loss: 65.72607871\n",
      "====> Epoch: 9 Average Training loss: 65.40284877\n",
      "====> Epoch: 9 Average Validation loss: 65.56918320\n",
      "====> Epoch: 10 Average Training loss: 65.23763596\n",
      "====> Epoch: 10 Average Validation loss: 65.41882891\n",
      "====> Epoch: 11 Average Training loss: 65.09306902\n",
      "====> Epoch: 11 Average Validation loss: 65.37518389\n",
      "====> Epoch: 12 Average Training loss: 65.06184431\n",
      "====> Epoch: 12 Average Validation loss: 65.32290176\n",
      "====> Epoch: 13 Average Training loss: 64.88152970\n",
      "====> Epoch: 13 Average Validation loss: 65.12727813\n",
      "====> Epoch: 14 Average Training loss: 64.79882263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/17 19:22:04 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 14 Average Validation loss: 65.11451030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/17 19:22:13 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.16.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torchvision==0.16.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "c:\\Users\\soupt\\anaconda3\\envs\\MPL\\lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\soupt\\anaconda3\\envs\\MPL\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 25)\n",
      "====> Epoch: 0 Average Training loss: 72.37756420\n",
      "====> Epoch: 0 Average Validation loss: 71.47946104\n",
      "====> Epoch: 1 Average Training loss: 70.82036498\n",
      "====> Epoch: 1 Average Validation loss: 70.49105273\n",
      "====> Epoch: 2 Average Training loss: 70.05188915\n",
      "====> Epoch: 2 Average Validation loss: 69.96459355\n",
      "====> Epoch: 3 Average Training loss: 69.63948443\n",
      "====> Epoch: 3 Average Validation loss: 69.57502627\n",
      "====> Epoch: 4 Average Training loss: 69.25108210\n",
      "====> Epoch: 4 Average Validation loss: 69.26501191\n",
      "====> Epoch: 5 Average Training loss: 68.91218867\n",
      "====> Epoch: 5 Average Validation loss: 69.04093613\n",
      "====> Epoch: 6 Average Training loss: 68.63095239\n",
      "====> Epoch: 6 Average Validation loss: 68.70655322\n",
      "====> Epoch: 7 Average Training loss: 67.87415828\n",
      "====> Epoch: 7 Average Validation loss: 67.44739648\n",
      "====> Epoch: 8 Average Training loss: 67.13601527\n",
      "====> Epoch: 8 Average Validation loss: 67.17347832\n",
      "====> Epoch: 9 Average Training loss: 66.89234175\n",
      "====> Epoch: 9 Average Validation loss: 68.61769434\n",
      "====> Epoch: 10 Average Training loss: 66.72557781\n",
      "====> Epoch: 10 Average Validation loss: 66.84111592\n",
      "====> Epoch: 11 Average Training loss: 66.55191744\n",
      "====> Epoch: 11 Average Validation loss: 66.84530889\n",
      "====> Epoch: 12 Average Training loss: 66.40766689\n",
      "====> Epoch: 12 Average Validation loss: 66.63254844\n",
      "====> Epoch: 13 Average Training loss: 66.31758556\n",
      "====> Epoch: 13 Average Validation loss: 66.54455674\n",
      "====> Epoch: 14 Average Training loss: 66.20616386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/17 19:28:37 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 14 Average Validation loss: 66.39101328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/17 19:28:47 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.16.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torchvision==0.16.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "c:\\Users\\soupt\\anaconda3\\envs\\MPL\\lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\soupt\\anaconda3\\envs\\MPL\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 25)\n",
      "====> Epoch: 0 Average Training loss: 70.45414444\n",
      "====> Epoch: 0 Average Validation loss: 68.64343037\n",
      "====> Epoch: 1 Average Training loss: 67.90581362\n",
      "====> Epoch: 1 Average Validation loss: 67.53840703\n",
      "====> Epoch: 2 Average Training loss: 67.09022186\n",
      "====> Epoch: 2 Average Validation loss: 67.10429668\n",
      "====> Epoch: 3 Average Training loss: 66.64288443\n",
      "====> Epoch: 3 Average Validation loss: 66.74046079\n",
      "====> Epoch: 4 Average Training loss: 66.32134214\n",
      "====> Epoch: 4 Average Validation loss: 66.53101572\n",
      "====> Epoch: 5 Average Training loss: 66.06320699\n",
      "====> Epoch: 5 Average Validation loss: 66.35591216\n",
      "====> Epoch: 6 Average Training loss: 65.85816267\n",
      "====> Epoch: 6 Average Validation loss: 66.00303564\n",
      "====> Epoch: 7 Average Training loss: 65.67679004\n",
      "====> Epoch: 7 Average Validation loss: 65.92229624\n",
      "====> Epoch: 8 Average Training loss: 65.46291150\n",
      "====> Epoch: 8 Average Validation loss: 65.70338423\n",
      "====> Epoch: 9 Average Training loss: 65.32175934\n",
      "====> Epoch: 9 Average Validation loss: 65.61756401\n",
      "====> Epoch: 10 Average Training loss: 65.17314080\n",
      "====> Epoch: 10 Average Validation loss: 65.47501934\n",
      "====> Epoch: 11 Average Training loss: 65.05256709\n",
      "====> Epoch: 11 Average Validation loss: 65.40909595\n",
      "====> Epoch: 12 Average Training loss: 64.97550327\n",
      "====> Epoch: 12 Average Validation loss: 65.39782085\n",
      "====> Epoch: 13 Average Training loss: 64.93453691\n",
      "====> Epoch: 13 Average Validation loss: 65.15367505\n",
      "====> Epoch: 14 Average Training loss: 64.83554256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/17 19:36:36 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 14 Average Validation loss: 65.09967095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/17 19:36:48 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.16.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torchvision==0.16.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "c:\\Users\\soupt\\anaconda3\\envs\\MPL\\lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\soupt\\anaconda3\\envs\\MPL\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 50)\n",
      "====> Epoch: 0 Average Training loss: 71.26241223\n",
      "====> Epoch: 0 Average Validation loss: 69.70701406\n",
      "====> Epoch: 1 Average Training loss: 68.67698563\n",
      "====> Epoch: 1 Average Validation loss: 68.16436973\n",
      "====> Epoch: 2 Average Training loss: 67.69446395\n",
      "====> Epoch: 2 Average Validation loss: 67.52249775\n",
      "====> Epoch: 3 Average Training loss: 67.12695942\n",
      "====> Epoch: 3 Average Validation loss: 67.08659102\n",
      "====> Epoch: 4 Average Training loss: 66.72908318\n",
      "====> Epoch: 4 Average Validation loss: 66.82385811\n",
      "====> Epoch: 5 Average Training loss: 66.45163842\n",
      "====> Epoch: 5 Average Validation loss: 66.53947559\n",
      "====> Epoch: 6 Average Training loss: 66.19764998\n",
      "====> Epoch: 6 Average Validation loss: 66.45412295\n",
      "====> Epoch: 7 Average Training loss: 65.99848103\n",
      "====> Epoch: 7 Average Validation loss: 66.21177842\n",
      "====> Epoch: 8 Average Training loss: 65.79878806\n",
      "====> Epoch: 8 Average Validation loss: 66.05497803\n",
      "====> Epoch: 9 Average Training loss: 65.65715591\n",
      "====> Epoch: 9 Average Validation loss: 65.84673125\n",
      "====> Epoch: 10 Average Training loss: 65.54058653\n",
      "====> Epoch: 10 Average Validation loss: 65.86526504\n",
      "====> Epoch: 11 Average Training loss: 65.38448861\n",
      "====> Epoch: 11 Average Validation loss: 65.69221494\n",
      "====> Epoch: 12 Average Training loss: 65.21532183\n",
      "====> Epoch: 12 Average Validation loss: 65.60384072\n",
      "====> Epoch: 13 Average Training loss: 65.12972320\n",
      "====> Epoch: 13 Average Validation loss: 65.42773506\n",
      "====> Epoch: 14 Average Training loss: 65.05975461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/17 19:43:55 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 14 Average Validation loss: 65.44846514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/17 19:44:05 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.16.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torchvision==0.16.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "c:\\Users\\soupt\\anaconda3\\envs\\MPL\\lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\soupt\\anaconda3\\envs\\MPL\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 50)\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from hyperparam_optim import HyperparameterOptimization\n",
    "from tracking import MLFlowTracker\n",
    "import itertools\n",
    "from visualize import visualize_latent_space_Eqn\n",
    "\n",
    "hyperParamOptimization = HyperparameterOptimization('./hyperparameters/params.yaml')\n",
    "params = hyperParamOptimization.get_params()\n",
    "\n",
    "#mlflow.create_experiment('Character AE Equation V1')\n",
    "mlflow.set_experiment('Character AE Equation V1')\n",
    "\n",
    "\n",
    "hyperparameter_combinations = itertools.product(params.M_LATENTDIMENSION, params.T_NUMEPOCHS, params.T_LR, params.T_BATCHSIZE, params.T_LOSS, params.T_OPTIMIZER, params.T_TRAINVALIDRATIO)\n",
    "for combination in hyperparameter_combinations:\n",
    "    mlflow.start_run()\n",
    "    latent_dim, num_epochs, lr, batch_size, loss_function, optimizer_name, train_valid_ratio = combination\n",
    "\n",
    "    # Initialize your model with the given latent dimension\n",
    "    # Model Initialization\n",
    "    alphabet_length = len(alphabet) + 1\n",
    "    MAX_SEQ_LEN = 21\n",
    "    model = EqnAE(alphabet_length,MAX_SEQ_LEN,latent_rep_size=latent_dim)\n",
    "    model.to(device)\n",
    "    #Loss\n",
    "    if loss_function == 'BCE':\n",
    "        loss_module = nn.BCELoss(reduction=\"sum\")\n",
    "    elif loss_function == 'MSE':\n",
    "        loss_module = nn.MSELoss(reduction=\"sum\")\n",
    "    \n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'Adadelta':\n",
    "        optimizer = torch.optim.Adadelta(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'Adagrad':\n",
    "        optimizer = torch.optim.Adagrad(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(),lr=lr)\n",
    "\n",
    "    total_samples = one_hot_encoded_training_tensor.shape[0]\n",
    "    valid_size = int(total_samples * train_valid_ratio)\n",
    "    train_size = total_samples - valid_size\n",
    "\n",
    "    train_tensor = one_hot_encoded_training_tensor[:train_size]\n",
    "    valid_tensor = one_hot_encoded_training_tensor[train_size:]\n",
    "\n",
    "    train_dataset = TensorDataset(train_tensor)\n",
    "    val_dataset = TensorDataset(valid_tensor) \n",
    "\n",
    "    train_loader = DataLoader(dataset=train_tensor,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False)\n",
    "\n",
    "    val_loader = DataLoader(dataset=valid_tensor,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False)\n",
    "\n",
    "    # Log parameters to MLflow\n",
    "    mlflow.log_params({\n",
    "        \"latent_dimension\": latent_dim,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": lr,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"loss_function\": loss_function,\n",
    "        \"optimizer\": optimizer_name,\n",
    "        \"train_valid_ratio\": train_valid_ratio\n",
    "    })\n",
    "\n",
    "    train_EqnAE(model, train_loader, val_loader, loss_module, optimizer, num_epochs=num_epochs)\n",
    "\n",
    "    \n",
    "    torch.save(model,f'./saved/models/EQN_AE_{latent_dim}_{batch_size}_{loss_function}_{lr}_{num_epochs}_{optimizer_name}_{train_valid_ratio}_Full.pth')\n",
    "    torch.save(model.state_dict(),f'./saved/models/EQN_AE_{latent_dim}_{batch_size}_{loss_function}_{lr}_{num_epochs}_{optimizer_name}_{train_valid_ratio}_dict.pth')\n",
    "\n",
    "    # Save the model\n",
    "    mlflow.pytorch.log_model(model, \"model\")\n",
    "\n",
    "    test_tensor = one_hot_encoded_testing_tensor\n",
    "    test_dataset = TensorDataset(test_tensor)\n",
    "    test_loader = DataLoader(dataset=test_tensor,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False)             \n",
    "    model.to(device)\n",
    "    visualize_latent_space_Eqn(model, test_loader,False)\n",
    "    mlflow.log_artifact(\"./plots/LatentSpace_plot.png\", artifact_path=\"plots\")\n",
    "\n",
    "    one_hot_decoded = []\n",
    "    one_hot_decoded_recon = []\n",
    "    for sample in test_loader:\n",
    "        model.eval()\n",
    "        sample = sample.float().to(device)\n",
    "        recon = model(sample)\n",
    "        for idx,ele in enumerate(recon):\n",
    "            max_indices = torch.argmax(ele, dim=1)\n",
    "            one_hot = torch.zeros_like(ele) \n",
    "            one_hot[torch.arange(ele.size(0)), max_indices] = 1\n",
    "            embd = torch.argmax(one_hot, dim=1)\n",
    "            one_hot_decoded.append(emb.decode(torch.argmax(sample[idx], dim=1)))\n",
    "            one_hot_decoded_recon.append(emb.decode(embd))\n",
    "            #break\n",
    "\n",
    "        output_filename = f\"./output/output_{latent_dim}_{batch_size}_{loss_function}_{lr}_{num_epochs}_{optimizer_name}_{train_valid_ratio}.txt\"\n",
    "        with open(output_filename, \"w\") as file:\n",
    "            for idx, ele in enumerate(one_hot_decoded_recon[:1000]):\n",
    "                actual_equation = ''.join(one_hot_decoded[idx])\n",
    "                decoded_equation = ''.join(one_hot_decoded_recon[idx])\n",
    "                file.write(f'Actual Equation: {actual_equation}\\n')\n",
    "                file.write(f'AE Decoded Equation: {decoded_equation}\\n\\n')\n",
    "\n",
    "        # Log the file as an artifact in MLflow\n",
    "        mlflow.log_artifact(output_filename, artifact_path=\"outputs\")\n",
    "\n",
    "    # End MLflow run\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding and visualizing some outputs:\n",
    "+ One hot decode into embedding and then use idxtotoken to convert to equations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import EqnVAE\n",
    "from train import train_EqnVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization and Hyperparameter Optimization for Character VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 0 Average Training loss: 1511.38888993\n",
      "====> Epoch: 0 Average Validation loss: 1493.05335156\n",
      "====> Epoch: 1 Average Training loss: 1484.67769792\n",
      "====> Epoch: 1 Average Validation loss: 1479.08861719\n",
      "====> Epoch: 2 Average Training loss: 1464.33134740\n",
      "====> Epoch: 2 Average Validation loss: 1447.77856719\n",
      "====> Epoch: 3 Average Training loss: 1440.97789358\n",
      "====> Epoch: 3 Average Validation loss: 1437.83442969\n",
      "====> Epoch: 4 Average Training loss: 1432.01184878\n",
      "====> Epoch: 4 Average Validation loss: 1430.78165469\n",
      "====> Epoch: 5 Average Training loss: 1425.38336858\n",
      "====> Epoch: 5 Average Validation loss: 1424.74794688\n",
      "====> Epoch: 6 Average Training loss: 1420.55318559\n",
      "====> Epoch: 6 Average Validation loss: 1422.54631406\n",
      "====> Epoch: 7 Average Training loss: 1416.18032101\n",
      "====> Epoch: 7 Average Validation loss: 1418.36767656\n",
      "====> Epoch: 8 Average Training loss: 1412.38814896\n",
      "====> Epoch: 8 Average Validation loss: 1413.80091563\n",
      "====> Epoch: 9 Average Training loss: 1409.47536944\n",
      "====> Epoch: 9 Average Validation loss: 1410.35605625\n",
      "====> Epoch: 10 Average Training loss: 1404.41744965\n",
      "====> Epoch: 10 Average Validation loss: 1408.99781562\n",
      "====> Epoch: 11 Average Training loss: 1401.15020139\n",
      "====> Epoch: 11 Average Validation loss: 1403.48831250\n",
      "====> Epoch: 12 Average Training loss: 1398.16768906\n",
      "====> Epoch: 12 Average Validation loss: 1402.23606563\n",
      "====> Epoch: 13 Average Training loss: 1395.60296962\n",
      "====> Epoch: 13 Average Validation loss: 1398.27097344\n",
      "====> Epoch: 14 Average Training loss: 1394.86338247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/17 18:11:39 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 14 Average Validation loss: 1395.91384844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/17 18:11:48 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.16.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torchvision==0.16.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "c:\\Users\\soupt\\anaconda3\\envs\\MPL\\lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\soupt\\anaconda3\\envs\\MPL\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10)\n",
      "====> Epoch: 0 Average Training loss: 1527.13772118\n",
      "====> Epoch: 0 Average Validation loss: 1503.55567500\n",
      "====> Epoch: 1 Average Training loss: 1492.60958368\n",
      "====> Epoch: 1 Average Validation loss: 1484.62061875\n",
      "====> Epoch: 2 Average Training loss: 1480.64010243\n",
      "====> Epoch: 2 Average Validation loss: 1475.80614375\n",
      "====> Epoch: 3 Average Training loss: 1470.38492708\n",
      "====> Epoch: 3 Average Validation loss: 1463.65347500\n",
      "====> Epoch: 4 Average Training loss: 1453.28304375\n",
      "====> Epoch: 4 Average Validation loss: 1448.62960000\n",
      "====> Epoch: 5 Average Training loss: 1437.58108507\n",
      "====> Epoch: 5 Average Validation loss: 1435.78717500\n",
      "====> Epoch: 6 Average Training loss: 1430.59317257\n",
      "====> Epoch: 6 Average Validation loss: 1438.55752500\n",
      "====> Epoch: 7 Average Training loss: 1425.50766181\n",
      "====> Epoch: 7 Average Validation loss: 1427.79640625\n",
      "====> Epoch: 8 Average Training loss: 1421.25277708\n",
      "====> Epoch: 8 Average Validation loss: 1422.97734375\n",
      "====> Epoch: 9 Average Training loss: 1417.78245417\n",
      "====> Epoch: 9 Average Validation loss: 1425.76628437\n",
      "====> Epoch: 10 Average Training loss: 1414.77847396\n",
      "====> Epoch: 10 Average Validation loss: 1417.21802813\n",
      "====> Epoch: 11 Average Training loss: 1411.11144583\n",
      "====> Epoch: 11 Average Validation loss: 1415.60235313\n",
      "====> Epoch: 12 Average Training loss: 1406.22845069\n",
      "====> Epoch: 12 Average Validation loss: 1415.24687500\n",
      "====> Epoch: 13 Average Training loss: 1403.53061424\n",
      "====> Epoch: 13 Average Validation loss: 1405.34645312\n",
      "====> Epoch: 14 Average Training loss: 1400.36918854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/17 18:18:42 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 14 Average Validation loss: 1404.08303750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/17 18:18:52 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.16.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torchvision==0.16.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "c:\\Users\\soupt\\anaconda3\\envs\\MPL\\lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\soupt\\anaconda3\\envs\\MPL\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10)\n",
      "====> Epoch: 0 Average Training loss: 1491.41427604\n",
      "====> Epoch: 0 Average Validation loss: 1452.54155000\n",
      "====> Epoch: 1 Average Training loss: 1441.37228715\n",
      "====> Epoch: 1 Average Validation loss: 1436.66017500\n",
      "====> Epoch: 2 Average Training loss: 1427.61686042\n",
      "====> Epoch: 2 Average Validation loss: 1427.96544531\n",
      "====> Epoch: 3 Average Training loss: 1420.80996163\n",
      "====> Epoch: 3 Average Validation loss: 1420.84968594\n",
      "====> Epoch: 4 Average Training loss: 1413.54146701\n",
      "====> Epoch: 4 Average Validation loss: 1418.45469375\n",
      "====> Epoch: 5 Average Training loss: 1411.55951146\n",
      "====> Epoch: 5 Average Validation loss: 1411.22021094\n",
      "====> Epoch: 6 Average Training loss: 1406.19152448\n",
      "====> Epoch: 6 Average Validation loss: 1406.13718281\n",
      "====> Epoch: 7 Average Training loss: 1400.40074583\n",
      "====> Epoch: 7 Average Validation loss: 1401.24834844\n",
      "====> Epoch: 8 Average Training loss: 1394.76494167\n",
      "====> Epoch: 8 Average Validation loss: 1396.49165313\n",
      "====> Epoch: 9 Average Training loss: 1390.69093438\n",
      "====> Epoch: 9 Average Validation loss: 1392.38339375\n",
      "====> Epoch: 10 Average Training loss: 1386.09970859\n",
      "====> Epoch: 10 Average Validation loss: 1389.11975469\n",
      "====> Epoch: 11 Average Training loss: 1382.19129722\n",
      "====> Epoch: 11 Average Validation loss: 1387.05545000\n",
      "====> Epoch: 12 Average Training loss: 1379.75828915\n",
      "====> Epoch: 12 Average Validation loss: 1382.71193125\n",
      "====> Epoch: 13 Average Training loss: 1377.59032995\n",
      "====> Epoch: 13 Average Validation loss: 1380.58817344\n",
      "====> Epoch: 14 Average Training loss: 1374.31410634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/17 18:26:27 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 14 Average Validation loss: 1382.16915937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/17 18:26:36 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.16.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torchvision==0.16.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "c:\\Users\\soupt\\anaconda3\\envs\\MPL\\lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\soupt\\anaconda3\\envs\\MPL\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 25)\n",
      "====> Epoch: 0 Average Training loss: 1518.86048125\n",
      "====> Epoch: 0 Average Validation loss: 1485.71015625\n",
      "====> Epoch: 1 Average Training loss: 1464.44097396\n",
      "====> Epoch: 1 Average Validation loss: 1459.98221562\n",
      "====> Epoch: 2 Average Training loss: 1441.16833646\n",
      "====> Epoch: 2 Average Validation loss: 1439.43776875\n",
      "====> Epoch: 3 Average Training loss: 1430.66247361\n",
      "====> Epoch: 3 Average Validation loss: 1432.60267813\n",
      "====> Epoch: 4 Average Training loss: 1424.19275764\n",
      "====> Epoch: 4 Average Validation loss: 1426.05798437\n",
      "====> Epoch: 5 Average Training loss: 1418.86207222\n",
      "====> Epoch: 5 Average Validation loss: 1420.15964687\n",
      "====> Epoch: 6 Average Training loss: 1413.58630625\n",
      "====> Epoch: 6 Average Validation loss: 1416.11780000\n",
      "====> Epoch: 7 Average Training loss: 1408.56439583\n",
      "====> Epoch: 7 Average Validation loss: 1409.74767187\n",
      "====> Epoch: 8 Average Training loss: 1403.68623160\n",
      "====> Epoch: 8 Average Validation loss: 1405.28164687\n",
      "====> Epoch: 9 Average Training loss: 1399.08976632\n",
      "====> Epoch: 9 Average Validation loss: 1402.48034687\n",
      "====> Epoch: 10 Average Training loss: 1395.31166146\n",
      "====> Epoch: 10 Average Validation loss: 1398.90071250\n",
      "====> Epoch: 11 Average Training loss: 1392.44179132\n",
      "====> Epoch: 11 Average Validation loss: 1396.32662188\n",
      "====> Epoch: 12 Average Training loss: 1389.13972778\n",
      "====> Epoch: 12 Average Validation loss: 1394.35592500\n",
      "====> Epoch: 13 Average Training loss: 1386.49422187\n",
      "====> Epoch: 13 Average Validation loss: 1390.41597813\n",
      "====> Epoch: 14 Average Training loss: 1384.04036944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/17 18:33:10 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 14 Average Validation loss: 1389.72922812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/17 18:33:18 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.16.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torchvision==0.16.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "c:\\Users\\soupt\\anaconda3\\envs\\MPL\\lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\soupt\\anaconda3\\envs\\MPL\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 25)\n",
      "====> Epoch: 0 Average Training loss: 1498.08776528\n",
      "====> Epoch: 0 Average Validation loss: 1467.26437812\n",
      "====> Epoch: 1 Average Training loss: 1451.16631076\n",
      "====> Epoch: 1 Average Validation loss: 1444.28618437\n",
      "====> Epoch: 2 Average Training loss: 1436.02894566\n",
      "====> Epoch: 2 Average Validation loss: 1432.46808594\n",
      "====> Epoch: 3 Average Training loss: 1427.01081597\n",
      "====> Epoch: 3 Average Validation loss: 1427.63687656\n",
      "====> Epoch: 4 Average Training loss: 1419.88561962\n",
      "====> Epoch: 4 Average Validation loss: 1422.10073125\n",
      "====> Epoch: 5 Average Training loss: 1413.94719288\n",
      "====> Epoch: 5 Average Validation loss: 1415.26866563\n",
      "====> Epoch: 6 Average Training loss: 1409.57350417\n",
      "====> Epoch: 6 Average Validation loss: 1410.66543594\n",
      "====> Epoch: 7 Average Training loss: 1404.24584045\n",
      "====> Epoch: 7 Average Validation loss: 1405.11311094\n",
      "====> Epoch: 8 Average Training loss: 1398.61931910\n",
      "====> Epoch: 8 Average Validation loss: 1399.91033125\n",
      "====> Epoch: 9 Average Training loss: 1393.82736597\n",
      "====> Epoch: 9 Average Validation loss: 1395.70068438\n",
      "====> Epoch: 10 Average Training loss: 1391.23180990\n",
      "====> Epoch: 10 Average Validation loss: 1394.51884375\n",
      "====> Epoch: 11 Average Training loss: 1387.69027830\n",
      "====> Epoch: 11 Average Validation loss: 1390.38162812\n",
      "====> Epoch: 12 Average Training loss: 1385.14227925\n",
      "====> Epoch: 12 Average Validation loss: 1391.36481406\n",
      "====> Epoch: 13 Average Training loss: 1382.69275000\n",
      "====> Epoch: 13 Average Validation loss: 1386.42730938\n",
      "====> Epoch: 14 Average Training loss: 1380.80489332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/17 18:41:01 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 14 Average Validation loss: 1385.23392500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/17 18:41:11 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.16.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torchvision==0.16.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "c:\\Users\\soupt\\anaconda3\\envs\\MPL\\lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\soupt\\anaconda3\\envs\\MPL\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 50)\n",
      "====> Epoch: 0 Average Training loss: 1510.03329653\n",
      "====> Epoch: 0 Average Validation loss: 1481.05089375\n",
      "====> Epoch: 1 Average Training loss: 1459.86874896\n",
      "====> Epoch: 1 Average Validation loss: 1458.56119375\n",
      "====> Epoch: 2 Average Training loss: 1448.12656632\n",
      "====> Epoch: 2 Average Validation loss: 1451.78588750\n",
      "====> Epoch: 3 Average Training loss: 1441.82169479\n",
      "====> Epoch: 3 Average Validation loss: 1444.50431875\n",
      "====> Epoch: 4 Average Training loss: 1435.00737604\n",
      "====> Epoch: 4 Average Validation loss: 1437.10038437\n",
      "====> Epoch: 5 Average Training loss: 1429.89226562\n",
      "====> Epoch: 5 Average Validation loss: 1431.08982812\n",
      "====> Epoch: 6 Average Training loss: 1425.16328299\n",
      "====> Epoch: 6 Average Validation loss: 1427.43260313\n",
      "====> Epoch: 7 Average Training loss: 1420.23893854\n",
      "====> Epoch: 7 Average Validation loss: 1421.92926250\n",
      "====> Epoch: 8 Average Training loss: 1416.33798472\n",
      "====> Epoch: 8 Average Validation loss: 1419.99503750\n",
      "====> Epoch: 9 Average Training loss: 1412.85487292\n",
      "====> Epoch: 9 Average Validation loss: 1415.82620312\n",
      "====> Epoch: 10 Average Training loss: 1409.81190243\n",
      "====> Epoch: 10 Average Validation loss: 1413.09687188\n",
      "====> Epoch: 11 Average Training loss: 1406.89364236\n",
      "====> Epoch: 11 Average Validation loss: 1410.68934375\n",
      "====> Epoch: 12 Average Training loss: 1403.26249479\n",
      "====> Epoch: 12 Average Validation loss: 1405.60692812\n",
      "====> Epoch: 13 Average Training loss: 1399.62171250\n",
      "====> Epoch: 13 Average Validation loss: 1402.79907500\n",
      "====> Epoch: 14 Average Training loss: 1396.64265104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/17 18:48:05 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 14 Average Validation loss: 1400.67619375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/17 18:48:14 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.16.2+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torchvision==0.16.2' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "c:\\Users\\soupt\\anaconda3\\envs\\MPL\\lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\soupt\\anaconda3\\envs\\MPL\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 50)\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from hyperparam_optim import HyperparameterOptimization\n",
    "from tracking import MLFlowTracker\n",
    "import itertools\n",
    "from visualize import visualize_latent_space_Eqn\n",
    "import gc\n",
    "import time\n",
    "\n",
    "hyperParamOptimization = HyperparameterOptimization('./hyperparameters/params.yaml')\n",
    "params = hyperParamOptimization.get_params()\n",
    "\n",
    "#mlflow.create_experiment('Character VAE Equation V2')\n",
    "mlflow.set_experiment('Character VAE Equation V2')\n",
    "\n",
    "hyperparameter_combinations = itertools.product(params.M_LATENTDIMENSION, params.T_NUMEPOCHS, params.T_LR, params.T_BATCHSIZE, params.T_LOSS, params.T_OPTIMIZER, params.T_TRAINVALIDRATIO)\n",
    "for i,combination in enumerate(hyperparameter_combinations):\n",
    "    # if i <= 1:\n",
    "    #     continue\n",
    "    mlflow.start_run()\n",
    "    latent_dim, num_epochs, lr, batch_size, loss_function, optimizer_name, train_valid_ratio = combination\n",
    "\n",
    "    # Initialize your model with the given latent dimension\n",
    "    # Model Initialization\n",
    "    alphabet_length = len(alphabet) + 1\n",
    "    MAX_SEQ_LEN = 21\n",
    "    model = None\n",
    "    model = EqnVAE(alphabet_length,MAX_SEQ_LEN,latent_rep_size=latent_dim,recon_loss=loss_function)\n",
    "    model.to(device)\n",
    "    #Loss\n",
    "    vaeLoss = None\n",
    "    vaeLoss = model.vae_loss\n",
    "    optimizer = None\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'Adadelta':\n",
    "        optimizer = torch.optim.Adadelta(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'Adagrad':\n",
    "        optimizer = torch.optim.Adagrad(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(),lr=lr)\n",
    "\n",
    "    total_samples = one_hot_encoded_training_tensor.shape[0]\n",
    "    valid_size = int(total_samples * train_valid_ratio)\n",
    "    train_size = total_samples - valid_size\n",
    "\n",
    "    train_tensor = one_hot_encoded_training_tensor[:train_size].clone().to(device)\n",
    "    valid_tensor = one_hot_encoded_training_tensor[train_size:].clone().to(device)\n",
    "    train_loader, val_loade = None, None\n",
    "    train_loader = DataLoader(dataset=train_tensor,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False)\n",
    "\n",
    "    val_loader = DataLoader(dataset=valid_tensor,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False)\n",
    "\n",
    "    # Log parameters to MLflow\n",
    "    mlflow.log_params({\n",
    "        \"latent_dimension\": latent_dim,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": lr,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"loss_function\": loss_function,\n",
    "        \"optimizer\": optimizer_name,\n",
    "        \"train_valid_ratio\": train_valid_ratio\n",
    "    })\n",
    "\n",
    "    train_EqnVAE(model, train_loader, val_loader, vaeLoss, optimizer, num_epochs=num_epochs)\n",
    "\n",
    "    \n",
    "    torch.save(model,f'./saved/models/EQN_VAE_{latent_dim}_{batch_size}_{loss_function}_{lr}_{num_epochs}_{optimizer_name}_{train_valid_ratio}_Full.pth')\n",
    "    torch.save(model.state_dict(),f'./saved/models/EQN_VAE_{latent_dim}_{batch_size}_{loss_function}_{lr}_{num_epochs}_{optimizer_name}_{train_valid_ratio}_dict.pth')\n",
    "\n",
    "    # Save the model\n",
    "    mlflow.pytorch.log_model(model, \"model\")\n",
    "\n",
    "    test_tensor = one_hot_encoded_testing_tensor.clone().to(device)\n",
    "    test_dataset = TensorDataset(test_tensor)\n",
    "    test_loader = DataLoader(dataset=test_tensor,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False)             \n",
    "    model.to(device)\n",
    "    visualize_latent_space_Eqn(model, test_loader,True)\n",
    "    mlflow.log_artifact(\"./plots/LatentSpace_plot.png\", artifact_path=\"plots\")\n",
    "\n",
    "    one_hot_decoded = []\n",
    "    one_hot_decoded_recon = []\n",
    "    for sample in test_loader:\n",
    "        model.eval()\n",
    "        sample = sample.float().to(device)\n",
    "        recon, _, _ = model(sample)\n",
    "        for idx,ele in enumerate(recon):\n",
    "            max_indices = torch.argmax(ele, dim=1)\n",
    "            one_hot = torch.zeros_like(ele) \n",
    "            one_hot[torch.arange(ele.size(0)), max_indices] = 1\n",
    "            embd = torch.argmax(one_hot, dim=1)\n",
    "            one_hot_decoded.append(emb.decode(torch.argmax(sample[idx], dim=1)))\n",
    "            one_hot_decoded_recon.append(emb.decode(embd))\n",
    "            #break\n",
    "\n",
    "        output_filename = f\"./output/output_{latent_dim}_{batch_size}_{loss_function}_{lr}_{num_epochs}_{optimizer_name}_{train_valid_ratio}.txt\"\n",
    "        with open(output_filename, \"w\") as file:\n",
    "            for idx, ele in enumerate(one_hot_decoded_recon[:1000]):\n",
    "                actual_equation = ''.join(one_hot_decoded[idx])\n",
    "                decoded_equation = ''.join(one_hot_decoded_recon[idx])\n",
    "                file.write(f'Actual Equation: {actual_equation}\\n')\n",
    "                file.write(f'VAE Decoded Equation: {decoded_equation}\\n\\n')\n",
    "\n",
    "        # Log the file as an artifact in MLflow\n",
    "        mlflow.log_artifact(output_filename, artifact_path=\"outputs\")\n",
    "\n",
    "    \n",
    "    # Clear memory\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()  # Clear cached memory\n",
    "        torch.cuda.manual_seed_all(42)  # Re-seed\n",
    "\n",
    "    # Wait for 5-10 seconds before the next run\n",
    "    \n",
    "    # End MLflow run\n",
    "    mlflow.end_run()\n",
    "    time.sleep(5)  # Sleep for 5 seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed Equations to Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"S -> S '+' T\": 0,\n",
       " \"S -> S '*' T\": 1,\n",
       " \"S -> S '/' T\": 2,\n",
       " \"S -> S '-' T\": 3,\n",
       " 'S -> T': 4,\n",
       " \"T -> '(' S ')'\": 5,\n",
       " \"T -> 'sin' '(' S ')'\": 6,\n",
       " \"T -> 'exp' '(' S ')'\": 7,\n",
       " \"T -> 'cos' '(' S ')'\": 8,\n",
       " \"T -> 'sqrt' '(' S ')'\": 9,\n",
       " \"T -> 'log' '(' S ')'\": 10,\n",
       " \"T -> 'x'\": 11,\n",
       " \"T -> '1'\": 12,\n",
       " \"T -> '2'\": 13,\n",
       " \"T -> '3'\": 14,\n",
       " \"Nothing -> 'None'\": 15}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rules Embedding\n",
    "data = CFGEquationDataset()\n",
    "cfg = data.get_grammar()\n",
    "emb = RuleTokenEmbedding(cfg=cfg,one_hot_encode=True)\n",
    "emb.rule_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intializing Grammar Masks and Mask Index for Introducing into Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1], [0, 1], [0, 1], [0, 1], [1], [0], [0], [0], [0], [0], [0], [], [], [], [], []]\n",
      "[[1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Parsing the grammar to create masks and indices\n",
    "all_lhs = [a.lhs().symbol() for a in cfg.productions()]\n",
    "lhs_list = []\n",
    "for a in all_lhs:\n",
    "    if a not in lhs_list:\n",
    "        lhs_list.append(a)\n",
    "D = len(cfg.productions())\n",
    "\n",
    "rhs_map = [None] * D\n",
    "for i, production in enumerate(cfg.productions()):\n",
    "    rhs_map[i] = [lhs_list.index(b.symbol()) for b in production.rhs() if (isinstance(b, nltk.Nonterminal) and b.symbol()!='None')]\n",
    "\n",
    "print(rhs_map)\n",
    "\n",
    "masks = np.zeros((len(lhs_list), D))\n",
    "for i, lhs in enumerate(lhs_list):\n",
    "    masks[i] = [lhs == production.lhs().symbol() for production in cfg.productions()]\n",
    "\n",
    "print(masks)\n",
    "ind_of_ind = np.array([np.where(masks[:, i] == 1)[0][0] for i in range(masks.shape[1])])\n",
    "print(ind_of_ind)\n",
    "# Convert numpy arrays to torch tensors\n",
    "masks_tensor = torch.from_numpy(masks).float()\n",
    "ind_of_ind_tensor = torch.from_numpy(ind_of_ind).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Encoding of an equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       " array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eqn = ['sqrt','(','log','(','x','*','cos','(','2',')',')',')','*','x']\n",
    "encoded_gram_eqn = emb.embed(eqn)\n",
    "encoded_gram_eqn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammar VAE Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rules = len(cfg.productions())\n",
    "num_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n"
     ]
    }
   ],
   "source": [
    "from models import EqnGVAE\n",
    "from train import train_EqnGVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation for GVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_val_gvae_dataset = CFGEquationDataset(\n",
    "        n_samples=100000,\n",
    "        transform=Compose([\n",
    "            RuleTokenEmbedding(cfg,max_num_rules=16,one_hot_encode=True),\n",
    "            ToTensor(dtype=torch.uint8)\n",
    "        ]))\n",
    "\n",
    "#Batch Size:\n",
    "batch_size = 100\n",
    "MAX_PROD_LEN = 16\n",
    "\n",
    "test_gvae_dataset = CFGEquationDataset(\n",
    "        n_samples=1000,\n",
    "        transform=Compose([\n",
    "            RuleTokenEmbedding(cfg,max_num_rules=16,one_hot_encode=True),\n",
    "            ToTensor(dtype=torch.uint8)\n",
    "        ]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training of GVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\soupt\\OneDrive - stud.uni-stuttgart.de\\AI LAB\\Project\\Repo\\ac_dll_grammar_vae\\data\\transforms.py:50: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  return torch.tensor(x, dtype=self.dtype)\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from hyperparam_optim import HyperparameterOptimization\n",
    "from tracking import MLFlowTracker\n",
    "import itertools\n",
    "from visualize import visualize_latent_space_Eqn\n",
    "import gc\n",
    "import time\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "hyperParamOptimization = HyperparameterOptimization('./hyperparameters/params.yaml')\n",
    "params = hyperParamOptimization.get_params()\n",
    "\n",
    "#mlflow.create_experiment('Character VAE Equation V2')\n",
    "mlflow.set_experiment('Grammar VAE Equation V1')\n",
    "\n",
    "hyperparameter_combinations = itertools.product(params.M_LATENTDIMENSION, params.T_NUMEPOCHS, params.T_LR, params.T_BATCHSIZE, params.T_LOSS, params.T_OPTIMIZER, params.T_TRAINVALIDRATIO)\n",
    "for i,combination in enumerate(hyperparameter_combinations):\n",
    "    # if i <= 1:\n",
    "    #     continue\n",
    "    mlflow.start_run()\n",
    "    latent_dim, num_epochs, lr, batch_size, loss_function, optimizer_name, train_valid_ratio = combination\n",
    "\n",
    "    # Initialize your model with the given latent dimension\n",
    "    # Model Initialization\n",
    "    model = None\n",
    "    model = EqnGVAE(num_rules,MAX_PROD_LEN,masks_tensor=masks_tensor,ind_of_masks=ind_of_ind_tensor)\n",
    "    model.to(device)\n",
    "    #Loss\n",
    "    vaeLoss = None\n",
    "    vaeLoss = model.vae_loss\n",
    "    optimizer = None\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'Adadelta':\n",
    "        optimizer = torch.optim.Adadelta(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'Adagrad':\n",
    "        optimizer = torch.optim.Adagrad(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(),lr=lr)\n",
    "\n",
    "\n",
    "\n",
    "    total_samples = len(training_val_gvae_dataset)\n",
    "    valid_size = int(total_samples * train_valid_ratio)\n",
    "    train_size = total_samples - valid_size\n",
    "\n",
    "    train_dataset, valid_dataset = random_split(training_val_gvae_dataset, [train_size, valid_size])\n",
    "    train_loader, val_loader = None, None\n",
    "    train_loader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "\n",
    "    val_loader = DataLoader(dataset=valid_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "\n",
    "    # Log parameters to MLflow\n",
    "    mlflow.log_params({\n",
    "        \"latent_dimension\": latent_dim,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": lr,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"loss_function\": loss_function,\n",
    "        \"optimizer\": optimizer_name,\n",
    "        \"train_valid_ratio\": train_valid_ratio\n",
    "    })\n",
    "\n",
    "    train_EqnGVAE(model, train_loader, val_loader, vaeLoss, optimizer, num_epochs=num_epochs)\n",
    "\n",
    "    \n",
    "    torch.save(model,f'./saved/models/EQN_GVAE_{latent_dim}_{batch_size}_{loss_function}_{lr}_{num_epochs}_{optimizer_name}_{train_valid_ratio}_Full.pth')\n",
    "    torch.save(model.state_dict(),f'./saved/models/EQN_GVAE_{latent_dim}_{batch_size}_{loss_function}_{lr}_{num_epochs}_{optimizer_name}_{train_valid_ratio}_dict.pth')\n",
    "\n",
    "    # Save the model\n",
    "    mlflow.pytorch.log_model(model, \"model\")\n",
    "\n",
    "    test_dataset = test_gvae_dataset\n",
    "    test_loader = DataLoader(dataset=test_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False)             \n",
    "    model.to(device)\n",
    "    visualize_latent_space_EqnGVAE(model,test_loader,max_num_rules=16,cfg=cfg,vae=True)\n",
    "    mlflow.log_artifact(\"./plots/LatentSpace_plot.png\", artifact_path=\"plots\")\n",
    "\n",
    "    one_hot_decoded = []\n",
    "    one_hot_decoded_recon = []\n",
    "    for sample in test_loader:\n",
    "        model.eval()\n",
    "        sample = sample.float().to(device)\n",
    "        recon, _, _ = model(sample)\n",
    "        for idx,ele in enumerate(recon):\n",
    "            one_hot_decoded.append(emb.decode(sample.cpu().numpy()))\n",
    "            one_hot_decoded_recon.append(emb.decode(recon[i].cpu().detach().numpy()))\n",
    "            #break\n",
    "\n",
    "        output_filename = f\"./output/output_{latent_dim}_{batch_size}_{loss_function}_{lr}_{num_epochs}_{optimizer_name}_{train_valid_ratio}.txt\"\n",
    "        with open(output_filename, \"w\") as file:\n",
    "            for idx, ele in enumerate(one_hot_decoded_recon):\n",
    "                actual_equation = ''.join(one_hot_decoded[idx])\n",
    "                decoded_equation = ''.join(one_hot_decoded_recon[idx])\n",
    "                file.write(f'Actual Equation: {actual_equation}\\n')\n",
    "                file.write(f'GVAE Decoded Equation: {decoded_equation}\\n\\n')\n",
    "\n",
    "        # Log the file as an artifact in MLflow\n",
    "        mlflow.log_artifact(output_filename, artifact_path=\"outputs\")\n",
    "\n",
    "    \n",
    "    # Clear memory\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()  # Clear cached memory\n",
    "        torch.cuda.manual_seed_all(42)  # Re-seed\n",
    "\n",
    "    # Wait for 5-10 seconds before the next run\n",
    "    \n",
    "    # End MLflow run\n",
    "    mlflow.end_run()\n",
    "    time.sleep(5)  # Sleep for 5 seconds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MPL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
